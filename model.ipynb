{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C63jJjvAbi4-"
      },
      "source": [
        "# Getting Started \n",
        "This section is based on the [Datathon Tutorial](https://colab.research.google.com/github/GoogleCloudPlatform/healthcare/blob/master/datathon/anzics18/tutorial.ipynb#scrollTo=BFOlLhuTE9OG) by Google. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmsWy8b5ykW_"
      },
      "source": [
        "Import all necessary Python libraries. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TP5ajKpRbh98"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.path as path\n",
        "import uuid\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import gensim\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import random\n",
        "import time\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Imports for accessing data using Google BigQuery.\n",
        "from google.colab import auth\n",
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcAsfY9L0XBn"
      },
      "source": [
        "Provide your credentials to the runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-akY2Lb0Xrh"
      },
      "outputs": [],
      "source": [
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvcuAofH1IHc"
      },
      "source": [
        "# Accessing the MIMIC-III Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL7yVAek1bHt"
      },
      "source": [
        "This section assumes that you have already followed [these instructions](https://mimic.mit.edu/docs/iii/tutorials/intro-to-mimic-iii-bq/) to get access to the MIMIC-III dataset in your personal GCP BigQuery instance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9zrWc6724yP"
      },
      "source": [
        "The data-hosting project `physionet-data` has read-only access. As a result, you need to set a default project that you have BigQuery access to. This notebook uses `cse-6250-project`. **You should create a project of this name in your Google Cloud instance or change the below project name to one that you have write access to.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFiWg-zq3ccg"
      },
      "outputs": [],
      "source": [
        "#@title Setting default project ID (you may need to create your own) {display-mode:\"both\"}\n",
        "project_id='cse-6250-project' # @param\n",
        "client = bigquery.Client(project=project_id)\n",
        "datasets = client.list_datasets()\n",
        "for dataset in datasets:\n",
        "  did = dataset.dataset_id\n",
        "  # Optional to verify that you are able to access the datasets.\n",
        "  # The datasets we care about are mimiciii_clinical and mimiciii_notes.\n",
        "  print(did) \n",
        "\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"]=project_id\n",
        "!gcloud config set project $project_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcI7WIgek45g"
      },
      "source": [
        "## Data Exploration\n",
        "\n",
        "1. Get basic statistics on the data\n",
        "2. Create a histogram on note length\n",
        "3. Create a histogram of note category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnCb0XnT_FE-"
      },
      "outputs": [],
      "source": [
        "# Create a histogram of category\n",
        "query = \"SELECT CATEGORY, count(*) AS count FROM `mimiciii_notes.note_events` GROUP BY CATEGORY\"\n",
        "category_df = client.query(query).to_dataframe()\n",
        "\n",
        "category_df.plot.bar(x=\"CATEGORY\", y=\"count\")\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvGEsRCrIPnR"
      },
      "outputs": [],
      "source": [
        "# Create a histogram of note length\n",
        "query = \"SELECT LENGTH(TEXT) AS length FROM `mimiciii_notes.note_events`\"\n",
        "length_df = client.query(query).to_dataframe()\n",
        "\n",
        "plt.hist(length_df['length'], bins = 20)\n",
        "plt.xlabel('Note Length')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLZVIeKk_W0a"
      },
      "outputs": [],
      "source": [
        "query = \"SELECT TEXT FROM `mimiciii_notes.note_events` WHERE RAND() < 3/(SELECT COUNT(*) FROM `mimiciii_notes.note_events`)\"\n",
        "temp = client.query(query)\n",
        "notes = temp.to_dataframe()\n",
        "print(notes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIUGPzf-hDVP"
      },
      "source": [
        "## Feature Extraction with BigQuery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T__OVn8vbk0q"
      },
      "source": [
        "\n",
        "`ADMISSIONS`\n",
        "1.   Remove all newborn records ✅\n",
        "2.   Remove records that have a death date ✅\n",
        "3.   For each subject ID, add a new column to contain the next visit (basically linking each visit to the next visit. Will be used for prediction step) ✅\n",
        "4.   Filter out the ELECTIVE next admissions and backfilling ✅\n",
        "5.   Calculate the days until the next admission ✅\n",
        "6.   Create a chart of the number of days between admissions for the report ✅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARcnpAT4fXNi"
      },
      "outputs": [],
      "source": [
        "# Drops columns that aren't relevant, removes newborn admissions, and removes records with a death date\n",
        "# Note: if you are building off of this query, you may actually want to overwrite the existing table. \n",
        "# In that case, remove the part that says IF NOT EXISTS below \n",
        "table_id = \"mimiciii_notes.admissions_cleaned\"\n",
        "\n",
        "query = f\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS \n",
        "`{table_id}` (\n",
        "  `ROW_ID` INTEGER,\n",
        "  `SUBJECT_ID` INTEGER,\n",
        "  `HADM_ID` INTEGER,\n",
        "  `ADMITTIME` DATETIME,\n",
        "  `DISCHTIME` DATETIME,\n",
        "  `DEATHTIME` DATETIME,\n",
        "  `ADMISSION_TYPE` STRING\n",
        ")\n",
        "AS SELECT ROW_ID,SUBJECT_ID, HADM_ID, ADMITTIME, DISCHTIME, DEATHTIME, ADMISSION_TYPE \n",
        " FROM `mimiciii_notes.admissions` \n",
        " WHERE ADMISSION_TYPE != 'NEWBORN'\n",
        " AND DEATHTIME IS NULL\n",
        "\"\"\"\n",
        "\n",
        "qry = client.query(query)#.to_dataframe()\n",
        "assert qry.exception() is None\n",
        "\n",
        "table = client.get_table(table_id)\n",
        "assert table is not None\n",
        "\n",
        "print(\"Number of records: \" + str(table.num_rows))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4gV2n6JHSH2"
      },
      "outputs": [],
      "source": [
        "# For each record get the fields from the next visit\n",
        "# And calculate the number of days til the next admission\n",
        "table_id = \"mimiciii_notes.admissions_next\"\n",
        "\n",
        "query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE\n",
        "`{table_id}` (\n",
        "  `ROW_ID` INTEGER,\n",
        "  `SUBJECT_ID` INTEGER,\n",
        "  `HADM_ID` INTEGER,\n",
        "  `ADMITTIME` DATETIME,\n",
        "  `DISCHTIME` DATETIME,\n",
        "  `DEATHTIME` DATETIME,\n",
        "  `ADMISSION_TYPE` STRING,\n",
        "  `NEXT_ADMIT_ID` INTEGER,\n",
        "  `NEXT_ADMIT_TIME` DATETIME,\n",
        "  `NEXT_ADMIT_TYPE` STRING,\n",
        "  `DAY_DIFF` INTEGER\n",
        ")\n",
        "AS \n",
        "(\n",
        "  SELECT x.*\n",
        "    ,a.ADMITTIME AS NEXT_ADMIT_TIME\n",
        "    ,a.ADMISSION_TYPE AS NEXT_ADMIT_TYPE\n",
        "    ,DATE_DIFF(a.DISCHTIME, x.ADMITTIME, DAY)\n",
        "  FROM\n",
        "  (\n",
        "    SELECT a1.ROW_ID, a1.SUBJECT_ID, a1.HADM_ID, a1.ADMITTIME, a1.DISCHTIME, a1.DEATHTIME, a1.ADMISSION_TYPE\n",
        "      ,ARRAY_AGG(a2.HADM_ID ORDER BY  a2.ADMITTIME LIMIT 1)[SAFE_OFFSET(0)] AS NEXT_ADMIT_ID\n",
        "    FROM `mimiciii_notes.admissions_cleaned` a1\n",
        "    LEFT JOIN `mimiciii_notes.admissions_cleaned` a2 ON a1.SUBJECT_ID = a2.SUBJECT_ID AND a2.ADMISSION_TYPE NOT IN ('ELECTIVE','NEWBORN') AND a2.ADMITTIME > a1.ADMITTIME\n",
        "    GROUP BY a1.ROW_ID, a1.SUBJECT_ID, a1.HADM_ID, a1.ADMITTIME, a1.DISCHTIME, a1.DEATHTIME, a1.ADMISSION_TYPE\n",
        "  ) x\n",
        "  LEFT JOIN `mimiciii_notes.admissions_cleaned` a ON x.NEXT_ADMIT_ID = a.HADM_ID\n",
        ")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "qry = client.query(query)#.to_dataframe()\n",
        "assert qry.exception() is None\n",
        "\n",
        "table = client.get_table(table_id)\n",
        "assert table is not None\n",
        "\n",
        "print(\"Number of records: \" + str(table.num_rows))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akLi6qijzix5"
      },
      "outputs": [],
      "source": [
        "# Create a histogram of the number of days between admissions\n",
        "# Excludes the null values and limit to a year for readability\n",
        "query = \"SELECT * FROM `mimiciii_notes.admissions_next`\"\n",
        "dataframe = (client.query(query).result().to_dataframe())\n",
        "plt.hist(dataframe.loc[~dataframe.DAY_DIFF.isnull(),'DAY_DIFF'], bins=range(0,365,25))\n",
        "plt.xlim([0,365])\n",
        "plt.xlabel('Days til Next Admission')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-C-gk2gebxj"
      },
      "source": [
        "`NOTE_EVENTS`\n",
        "\n",
        "\n",
        "1.   Deal with duplicates—multiple records per patient, per visit (see the linked paper for a more thorough description) ✅\n",
        "2.   Convert all notes to lower case ✅\n",
        "3.   Remove new line and return characters in the notes ✅\n",
        "4.   Replace any punctuation with a space ✅\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wtwd78ZdUQIL"
      },
      "outputs": [],
      "source": [
        "# There are multiple notes for each admission for each patient\n",
        "# Combine all the notes into a single text\n",
        "table_id = \"mimiciii_notes.note_events_cleaned\"\n",
        "\n",
        "query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE\n",
        "`{table_id}` (\n",
        "  `SUBJECT_ID` INTEGER,\n",
        "  `HADM_ID` INTEGER,\n",
        "  `TEXT` STRING\n",
        ")\n",
        "AS (\n",
        "  SELECT n.SUBJECT_ID\n",
        "    ,n.HADM_ID\n",
        "    ,string_agg(n.TEXT, \" \")\n",
        "  FROM `mimiciii_notes.note_events` n\n",
        "  WHERE n.CATEGORY = 'Discharge summary'\n",
        "  GROUP BY n.SUBJECT_ID, n.HADM_ID\n",
        "  )\n",
        " \n",
        "\"\"\"\n",
        "\n",
        "qry = client.query(query)#.to_dataframe()\n",
        "assert qry.exception() is None\n",
        "\n",
        "table = client.get_table(table_id)\n",
        "assert table is not None\n",
        "\n",
        "print(\"Number of records: \" + str(table.num_rows))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tFxvp7o1ypy"
      },
      "source": [
        "Feature Engineering\n",
        "\n",
        "1.    Combine the note events and admissions tables ✅\n",
        "2.    Finish any necessary data cleaning for the notes in pandas ✅\n",
        "3.    Create a field for the target variable - 1 if readmitted within **45** days and 0 otherwise ✅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e08rbSTA4Hty"
      },
      "outputs": [],
      "source": [
        "# Join the note events and admission tables\n",
        "# Remove any admissions that don't have notes associated with it\n",
        "table_id = \"mimiciii_notes.admissions_notes_comb\"\n",
        "\n",
        "query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE\n",
        "`{table_id}` (\n",
        "  `SUBJECT_ID` INTEGER,\n",
        "  `HADM_ID` INTEGER,\n",
        "  `DAY_DIFF` INTEGER,\n",
        "  `TEXT` STRING\n",
        ")\n",
        "AS (\n",
        "  SELECT a.SUBJECT_ID\n",
        "    ,a.HADM_ID\n",
        "    ,a.DAY_DIFF\n",
        "    ,n.TEXT\n",
        "  FROM `mimiciii_notes.admissions_next` a\n",
        "  INNER JOIN `mimiciii_notes.note_events_cleaned` n ON a.HADM_ID  = n.HADM_ID\n",
        "  )\n",
        "\"\"\"\n",
        "\n",
        "qry = client.query(query)#.to_dataframe()\n",
        "assert qry.exception() is None\n",
        "\n",
        "table = client.get_table(table_id)\n",
        "assert table is not None\n",
        "\n",
        "print(\"Number of records: \" + str(table.num_rows))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YM6YLaBLEbY"
      },
      "outputs": [],
      "source": [
        "# Save to a pandas dataframe\n",
        "query = \"SELECT * FROM `mimiciii_notes.admissions_notes_comb`\"\n",
        "df = (client.query(query).result().to_dataframe())\n",
        "print(len(df))\n",
        "print(df.info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAFDxYPUDbnR"
      },
      "outputs": [],
      "source": [
        "# Finish data cleaning of the notes\n",
        "# Convert all notes to lower case\n",
        "df['TEXT'] = df['TEXT'].str.lower()\n",
        "# Remove new line and return characters or punctuation with a space\n",
        "df['TEXT'] = df['TEXT'].str.replace('\\n', ' ')\n",
        "df['TEXT'] = df['TEXT'].str.replace('\\r', ' ')\n",
        "df['TEXT'] = df['TEXT'].str.replace('\\W', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QghJQddPNgnc"
      },
      "outputs": [],
      "source": [
        "# Creature the target variable - 1 if the days between admissions is less than 45 and 0 otherwise\n",
        "df['TARGET'] = np.where(df['DAY_DIFF'] <= 45, 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9ucVtuKROoN"
      },
      "outputs": [],
      "source": [
        "#save df to google drive to save time\n",
        "saveFilePickle = False\n",
        "if saveFilePickle:\n",
        "  from google.colab import drive\n",
        "  import pickle\n",
        "  drive.mount('/content/drive/')\n",
        "  df.to_pickle('/content/drive/MyDrive/CSE6250/df.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih3CHP3S2QFN"
      },
      "source": [
        "## NLP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3C7fffbTwj_y"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "stop_words = ['the','and','to','of','was','with','a','on','in','for','name',\n",
        "            'is','patient','s','he','at','as','or','one','she','his','her','am',\n",
        "            'were','you','pt','pm','by','be','had','your','this','date',\n",
        "            'from','there','an','that','p','are','have','has','h','but','o',\n",
        "            'namepattern','which','every','also','should','if','it','been','who','during']\n",
        "\n",
        "def tokenize_note(text):\n",
        "\n",
        "    def tokenizer_helper(text):        \n",
        "        remove_list = string.punctuation+'0123456789'\n",
        "        t = str.maketrans(dict.fromkeys(remove_list, \" \"))\n",
        "        text = text.lower().translate(t)\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "        return tokens\n",
        "\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text):\n",
        "        for word in tokenizer_helper(sent):\n",
        "            if len(word) >= 2:\n",
        "              tokens.append(word.lower())\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2CSho84gc4r"
      },
      "source": [
        "\n",
        "## Free up space before modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxfvrDNTgk_h"
      },
      "outputs": [],
      "source": [
        "#deleting unused variables and free up RAM\n",
        "import gc, sys,psutil, pickle\n",
        "def get_memory_usage():\n",
        "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
        "def sizeof_fmt(num, suffix='B'):\n",
        "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
        "        if abs(num) < 1024.0:\n",
        "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
        "        num /= 1024.0\n",
        "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
        "for name, size in sorted(((name, sys.getsizeof(value)) for name,value in locals().items()),\n",
        "                         key= lambda x: -x[1])[:10]:\n",
        "    print(\"{:>30}: {:>8}\".format(name,sizeof_fmt(size)))\n",
        "print('Memory in Gb', get_memory_usage())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6LsPCCvgp08"
      },
      "outputs": [],
      "source": [
        "del dataframe\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7O0Czfi5KaU"
      },
      "source": [
        "## NLP and Modeling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFjrQCR5M9Em"
      },
      "outputs": [],
      "source": [
        "class model():\n",
        "    def __init__(self, df, textColumn = 'TEXT', target = 'TARGET', SEED = 2):\n",
        "        random.seed(SEED)\n",
        "        self.SEED = SEED\n",
        "        self.df = df.copy()\n",
        "      \n",
        "        #column with text\n",
        "        self.textColumn = textColumn\n",
        "    \n",
        "    #lem and remove stopwords\n",
        "    def lem(self, phrase):\n",
        "        stop_words_ = list(set(stopwords.words('english')))\n",
        "        wnl=WordNetLemmatizer()\n",
        "        lemmed=[]\n",
        "        for i in word_tokenize(phrase):\n",
        "            if i not in (stop_words_) and len(i) >= 2:\n",
        "                w1=wnl.lemmatize(i, 'v')\n",
        "                w2=wnl.lemmatize(w1, 'a')\n",
        "                w3=wnl.lemmatize(w2, 'n')\n",
        "                lemmed.append(w3.lower())\n",
        "        return \" \".join(lemmed)\n",
        "\n",
        "\n",
        "        #return \" \".join([wnl.lemmatize(words.lower(), pos=\"v\") for words in word_tokenize(phrase) if (words not in (stop_words_))])\n",
        "    \n",
        "    \n",
        "    \n",
        "    #remove spaces and underscores\n",
        "    def remove_spaces(self, phrase):\n",
        "        return  \" \".join(phrase.replace('_', '').split())\n",
        "\n",
        "\n",
        "    #adding word count as feature\n",
        "    def add_wordCount(self):\n",
        "        self.df['countLen']  = [len(tokenize_note(i)) for i in df[self.textColumn]]\n",
        "\n",
        "    def downsample (self, X, y):\n",
        "        # imbalanced classes so need to deal with that\n",
        "        # easiest and quickest way to deal with this is to sub-sample the negative records and create a balanced dataset\n",
        "        undersample = RandomUnderSampler(sampling_strategy=0.5)\n",
        "        X_under, y_under = undersample.fit_resample(X.values.reshape(-1,1), y)\n",
        "        return X_under, y_under\n",
        "\n",
        "\n",
        "    def preprocess(self, add_wordLen = False, downsample = False):\n",
        "        print('--------------------------------Sample Text-------------')\n",
        "        for i in self.df[self.textColumn].head(1):\n",
        "            print(i)\n",
        "        \n",
        "      \n",
        "\n",
        "        #30% test, 70% train\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test  =train_test_split(self.df[self.textColumn], self.df['TARGET'], test_size=0.3, random_state=self.SEED, stratify=self.df['TARGET'])\n",
        "\n",
        "\n",
        "        print('------------------------------Start Downsamping--------------------------')\n",
        "        self.X_train, self.y_train = self.downsample(self.X_train, self.y_train)\n",
        "        print('-----------------------------Finished Downsamping----------------------')\n",
        "        #self.X_train, self.X_val, self.y_train, self.y_val  = train_test_split(self.X_train, self.y_train, test_size=0.25, random_state=1, stratify=self.y_train)\n",
        "        \n",
        "\n",
        "\n",
        "        #remove spaces and underscores\n",
        "        self.X_train = [self.remove_spaces(x) for x in self.X_train.squeeze()]\n",
        "        self.X_test = self.X_test.apply(lambda x: self.remove_spaces(x))\n",
        "        print('------------------Sample Text after removing spaces and underscores----------------')\n",
        "        for i in self.X_train[:1]:\n",
        "            print(i)\n",
        "\n",
        "        print('---------------------------Start Lemmatization--------------------------------')\n",
        "        self.X_train = [self.lem(x) for x in self.X_train]\n",
        "        self.X_test = self.X_test.apply(lambda x: self.lem(x))\n",
        "        print('---------------------------------finished Lemmatize--------------------------')\n",
        "\n",
        "\n",
        "\n",
        "        print('Start Vectorizing')\n",
        "        #vectorize\n",
        "        vect_word = TfidfVectorizer(max_features=60000, lowercase=True, analyzer='word',tokenizer=tokenize_note,ngram_range=(1,3),dtype=np.float32)\n",
        "        self.tr_vect = vect_word.fit_transform(self.X_train)\n",
        "        self.vl_vect = vect_word.transform(self.X_test)\n",
        "        print('Finished Vectorizing')\n",
        "\n",
        "\n",
        "        print(self.tr_vect.shape[1] , 'total Width of the train vector' )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EStd4A43NJfh"
      },
      "outputs": [],
      "source": [
        "SEED = 2\n",
        "textColumnToUse = 'TEXT'\n",
        "target= 'TARGET'\n",
        "models = model(df, textColumnToUse, target, SEED)\n",
        "models.preprocess()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Model - Logistic Regression\n",
        "\n",
        "lgr = LogisticRegression(solver='saga',multi_class='multinomial', max_iter=1000, C=1,verbose = True, n_jobs = -1, class_weight='balanced', random_state = models.SEED).fit(models.tr_vect, models.y_train)\n",
        "t = time.process_time()\n",
        "lgr_y_pred = lgr.predict(models.vl_vect)\n",
        "elapsed_time = time.process_time() - t\n",
        "print('Total elappsed time: ', elapsed_time)\n",
        "print('LGR AUC:', metrics.roc_auc_score(models.y_test, lgr_y_pred))\n",
        "print(\"LGR Accuracy:\",metrics.accuracy_score(models.y_test, lgr_y_pred))\n",
        "print(\"LGR Precision:\",metrics.precision_score(models.y_test, lgr_y_pred))\n",
        "print(\"LGR Recall:\",metrics.recall_score(models.y_test, lgr_y_pred))"
      ],
      "metadata": {
        "id": "azhqJeWFq7TT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "disp = ConfusionMatrixDisplay.from_predictions(models.y_test, lgr_y_pred, cmap=plt.cm.Blues, display_labels=['Not Readmitted','Readmitted'])\n",
        "disp.ax_.set_title('Non-Normalized Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "disp = ConfusionMatrixDisplay.from_predictions(models.y_test, lgr_y_pred, cmap=plt.cm.Blues, display_labels=['Not Readmitted','Readmitted'], normalize = 'all')\n",
        "disp.ax_.set_title('Normalized Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aGZ0OEfGxS6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top features\n",
        "vect_word = TfidfVectorizer(max_features=60000, lowercase=True, analyzer='word',tokenizer=tokenize_note,ngram_range=(1,3),dtype=np.float32)\n",
        "tr_vect = vect_word.fit_transform(models.X_train)\n",
        "feature_names = np.array(vect_word.get_feature_names())\n",
        "importance_ind = np.argsort(np.asarray(tr_vect.sum(axis=0)).ravel())[::-1]\n",
        "importance_val = np.sort(np.asarray(tr_vect.sum(axis=0)).ravel())[::-1]\n"
      ],
      "metadata": {
        "id": "tqFRA8b61gBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 50\n",
        "top_words = feature_names[importance_ind[:n]]\n",
        "top_scores = importance_val[:n]\n",
        "bottom_words = feature_names[importance_ind[-n:]]\n",
        "bottom_scores = importance_val[-n:]\n",
        "\n",
        "y_position = np.arange(len(top_words))\n",
        "fig = plt.figure(figsize=(10, 10))  \n",
        "\n",
        "plt.subplot(121)\n",
        "plt.barh(y_position,bottom_scores, align='center', alpha=0.5)\n",
        "plt.title('Least Relevant', fontsize=20)\n",
        "plt.yticks(y_position, bottom_words, fontsize=14)\n",
        "plt.xlabel('Importance Score', fontsize=20)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.barh(y_position,top_scores, align='center', alpha=0.5)\n",
        "plt.title('Most Relevant', fontsize=20)\n",
        "plt.yticks(y_position, top_words, fontsize=14)\n",
        "plt.xlabel('Importance Score', fontsize=20)\n",
        "\n",
        "plt.subplots_adjust(wspace=1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "G0sqh4-l9wpE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}